'''The number of parameters (and consequently, the amount of training
data needed to adequately estimate them) would rapidly grow if we add more
features or higher order terms. This phenomenon, present in every machine learning
method, is called the idem curse of dimensionality: when the number of parameters
of a model grows, the data needed to learn them grows exponentially
'''
